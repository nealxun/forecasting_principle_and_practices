#ARIMA models {#ch8}

ARIMA models provide another approach to time series forecasting.
Exponential smoothing and ARIMA models are the two most widely-used
approaches to time series forecasting, and provide complementary
approaches to the problem. While exponential smoothing models were based
on a description of trend and seasonality in the data, ARIMA models aim
to describe the autocorrelations in the data.

Before we introduce ARIMA models, we need to first discuss the concept
of stationarity and the technique of differencing time series.

##Stationarity and differencing

**A stationary time series is one whose properties do not depend on the
time at which the series is observed.**[^1] So time series with trends,
or with seasonality, are not stationary --- the trend and seasonality will
affect the value of the time series at different times. On the other
hand, a white noise series is stationary --- it does not matter when you
observe it, it should look much the same at any period of time.

Some cases can be confusing --- a time series with cyclic behaviour (but
with no trend or seasonality) is stationary. That is because the cycles
are not of fixed length. So before we observe the series we cannot be
sure where the peaks and troughs of the cycles will be.

In general, a stationary time series will have no predictable patterns
in the long-term. Time plots will show the series to be roughly
horizontal (although some cyclic behaviour is possible) with constant
variance.

[!t] !\@ref(image)(stationary)

Figure : Which of these series are stationary? (a) Dow Jones index on
292 consecutive days; (b) Daily change in Dow Jones index on 292
consecutive days; (c) Annual number of strikes in the US; (d) Monthly
sales of new one-family houses sold in the US; (e) Annual price of a
dozen eggs in the US (constant dollars); (f) Monthly total of pigs
slaughtered in Victoria, Australia; (g) Annual total of lynx trapped in
the McKenzie River district of north-west Canada; (h) Monthly Australian
beer production; (i) Monthly Australian electricity production.

\@ref(fig-8-stationary)

Consider the nine series plotted in Figure \@ref(fig-8-stationary). Which of
these do you think are stationary?

Obvious seasonality rules out series (d), (h) and (i). Trend rules out
series (a), (c), (e), (f) and (i). Increasing variance also rules out
(i). That leaves only (b) and (g) as stationary series.

At first glance, the strong cycles in series (g) might appear to make it
non-stationary. But these cycles are aperiodic --- they are caused when
the lynx population becomes too large for the available feed, so they
stop breeding and the population falls to very low numbers, then the
regeneration of their food sources allows the population to grow again,
and so on. In the long-term, the timing of these cycles is not
predictable. Hence the series is stationary.

### Differencing  {-}{#differencing .unnumbered}

In Figure \@ref(fig-8-stationary), notice how the Dow Jones index data was
non-stationary in panel (a), but the daily changes were stationary in
panel (b). This shows one way to make a time series stationary --- compute
the differences between consecutive observations. This is known as
**differencing**.

Transformations such as logarithms can help to stabilize the variance of
a time series. Differencing can help stabilize the mean of a time series
by removing changes in the level of a time series, and so eliminating
(or reducing) trend and seasonality.

As well as looking at the time plot of the data, the ACF plot is also
useful for identifying non-stationary time series. For a stationary time
series, the ACF will drop to zero relatively quickly, while the ACF of
non-stationary data decreases slowly. Also, for non-stationary data, the
value of $r_1$ is often large and positive.

![The ACF of the Dow-Jones index (left) and of the daily changes in the
Dow-Jones index (right).](acfstationary)

\@ref(fig-8-acfstationary)

The ACF of the differenced Dow-Jones index looks just like that from a
<span>white noise series</span>. There is only one autocorrelation lying
just outside the 95% limits, and the Ljung-Box $Q^*$ statistic has a
p-value of 0.153 (for $h=10$). This suggests that the *daily change* in
the Dow-Jones index is essentially a random amount uncorrelated with
previous days.

### Random walk model  {-}{#random-walk-model .unnumbered}

The differenced series is the *change* between consecutive observations
in the original series, and can be written as $$y'_t = y_t - y_{t-1}.$$
The differenced series will have only $T-1$ values since it is not
possible to calculate a difference $y_1'$ for the first observation.

When the differenced series is white noise, the model for the original
series can be written as
$$y_t - y_{t-1} = e_t \quad\text{or}\quad {y_t = y_{t-1} + e_t}\: .$$ A
random walk model is very widely used for non-stationary data,
particularly finance and economic data. Random walks typically have:

-   long periods of apparent trends up or down

-   sudden and unpredictable changes in direction.

The forecasts from a random walk model are equal to the last
observation, as future movements are unpredictable, and are equally
likely to be up or down. Thus, the random walk model underpins naïve
forecasts.

A closely related model allows the differences to have a non-zero mean.
Then
$$y_t - y_{t-1} = c + e_t\quad\text{or}\quad {y_t = c + y_{t-1} + e_t}\: .$$
The value of $c$ is the average of the changes between consecutive
observations. If $c$ is positive, then the average change is an increase
in the value of $y_t$. Thus $y_t$ will tend to drift upwards. But if $c$
is negative, $y_t$ will tend to drift downwards.

This is the model behind the drift method discussed in
Section \@ref(sec-2-methods).

### Second {-}-order differencing {#second-order-differencing .unnumbered}

Occasionally the differenced data will not appear stationary and it may
be necessary to difference the data a second time to obtain a stationary
series:

$$\begin{aligned}
y''_{t}  &=  y'_{t}  - y'_{t - 1} \\
&= (y_t - y_{t-1}) - (y_{t-1}-y_{t-2})\\
&= y_t - 2y_{t-1} +y_{t-2}.\end{aligned}$$

In this case, $y_t''$ will have $T-2$ values. Then we would model the
“change in the changes” of the original data. In practice, it is almost
never necessary to go beyond second-order differences.

### Seasonal differencing  {-}{#seasonal-differencing .unnumbered}

A seasonal difference is the difference between an observation and the
corresponding observation from the previous year. So $$<span>y’~t~ =
y~t~ - y~t-m~</span> $$ These are also called “lag-$m$ differences” as
we subtract the observation after a lag of $m$ periods.

If seasonally differenced data appear to be white noise, then an
appropriate model for the original data is $$y_t = y_{t-m}+e_t.$$
Forecasts from this model are equal to the last observation from the
relevant season. That is, this model gives seasonal naïve forecasts.

![Logs and seasonal differences of the A10 (antidiabetic) sales data.
The logarithms stabilize the variance, while the seasonal differences
remove the seasonality and trend.](diffa10)

\@ref(fig-8-a10diff)

plot(a10, xlab=“Year”, ylab=“Sales (\$million)”, main=“Antidiabetic drug
sales”) plot(log(a10), xlab=“Year”, ylab=“Monthly log sales”)
229plot(diff(log(a10),12), xlab=“Year”, ylab=“Annual change in monthly
log sales”)

Figure \@ref(fig-8-a10diff) shows the seasonal differences of the logarithm
of the monthly scripts for A10 (antidiabetic) drugs sold in Australia.
The transformation and differencing has made the series look relatively
stationary.

To distinguish seasonal differences from ordinary differences, we
sometimes refer to ordinary differences as “first differences” meaning
differences at lag 1.

Sometimes it is necessary to do both a seasonal difference and a first
difference to obtain stationary data, as shown in
Figure \@ref(fig-8-diffusnetelec). Here, the data are first transformed using
logarithms (second panel). Then seasonal differenced are calculated
(third panel). The data still seem a little non-stationary, and so a
further lot of first differences are computed (bottom panel).

[!p]

!\@ref(image)(diffusnetelec)

\@ref(fig-8-diffusnetelec)

There is a degree of subjectivity in selecting what differences to
apply. The seasonally differenced data in Figure \@ref(fig-8-a10diff) do not
show substantially different behaviour from the seasonally differenced
data in Figure \@ref(fig-8-diffusnetelec). In the latter case, we may have
decided to stop with the seasonally differenced data, and not done an
extra round of differencing. In the former case, we may have decided the
data were not sufficiently stationary and taken an extra round of
differencing. Some formal tests for differencing will be discussed
later, but there are always some choices to be made in the modeling
process, and different analysts may make different choices.

If $y'_t = y_t - y_{t-m}$ denotes a seasonally differenced series, then
the twice-differenced series is

$$\begin{aligned}
y''_t &= y'_t - y'_{t-1} \\
      &= (y_t - y_{t-m}) - (y_{t-1} - y_{t-m-1}) \\
      &= y_t -y_{t-1} - y_{t-m} + y_{t-m-1}\: .\end{aligned}$$

When both seasonal and first differences are applied, it makes no
difference which is done first---the result will be the same. However, if
the data have a strong seasonal pattern, we recommend that seasonal
differencing be done first because sometimes the resulting series will
be stationary and there will be no need for a further first difference.
If first differencing is done first, there will still be seasonality
present.

It is important that if differencing is used, the differences are
interpretable. First differences are the change between **one
observation and the next**. Seasonal differences are the change between
**one year to the next**. Other lags are unlikely to make much
interpretable sense and should be avoided.

### Unit root tests  {-}{#unit-root-tests .unnumbered}

One way to determine more objectively if differencing is required is to
use a *unit root test*. These are statistical hypothesis tests of
stationarity that are designed for determining whether differencing is
required.

A number of unit root tests are available, and they are based on
different assumptions and may lead to conflicting answers.

One of the most popular tests is the *Augmented Dickey-Fuller (ADF)
test*. For this test, the following regression model is estimated:
$$y'_t = \phi y_{t-1} + \beta_1 y'_{t-1} + \beta_2 y'_{t-2} + \cdots + \beta_k y'_{t-k},$$
where $y'_t$ denotes the first-differenced series, $y'_t=y_t-y_{t-1}$
and $k$ is the number of lags to include in the regression (often set to
be about 3). If the original series, $y_t$, needs differencing, then the
coefficient $\hat\phi$ should be approximately zero. If $y_t$ is already
stationary, then $\hat{\phi}<0$. The usual hypothesis tests for
regression coefficients do not work when the data are non-stationary,
but the test can be carried out using the following R command.

adf.test(x, alternative = “stationary”)

In R, the default value of $k$ is set to $\lfloor(T - 1)^{1/3}\rfloor$
where $T$ is the length of the time series and $\lfloor x\rfloor$ means
the largest integer not greater than $x$.

The null-hypothesis for an ADF test is that the data are non-stationary.
So large p-values are indicative of non-stationarity, and small p-values
suggest stationarity. Using the usual 5% threshold, differencing is
required if the p-value is greater than 0.05.

Another popular unit root test is the *Kwiatkowski-Phillips-Schmidt-Shin
(KPSS) test*. This reverses the hypotheses, so the null-hypothesis is
that the data are stationary. In this case, small p-values (e.g., less
than 0.05) suggest that differencing is required.

kpss.test(x)

A useful R function is `ndiffs()` which uses these tests to determine
the appropriate number of first differences required for a non-seasonal
time series.

More complicated tests are required for seasonal differencing and are
beyond the scope of this book. A useful R function for determining
whether seasonal differencing is required is `nsdiffs()` which uses
seasonal unit root tests to determine the appropriate number of seasonal
differences required.

The following code can be used to find how to make a seasonal series
stationary. The resulting series stored as `xstar` has been differenced
appropriately.

ns \<- nsdiffs(x) if(ns \> 0) <span> xstar \<-
diff(x,lag=frequency(x),differences=ns) </span> else <span> xstar \<- x
</span> nd \<- ndiffs(xstar) if(nd \> 0) <span> xstar \<-
diff(xstar,differences=nd) </span>

#Backshift notation

The backward shift operator $B$ is a useful notational device when
working with time series lags: $${B y_{t} = y_{t - 1}} \: .$$ (Some
references use $L$ for “lag” instead of $B$ for “backshift”.) In other
words, $B$, operating on $y_{t}$, has the effect of **shifting the data
back one period**. Two applications of $B$ to $y_{t}$ **shifts the data
back two periods**: $$B(By_{t}) = B^{2}y_{t} = y_{t-2}\: .$$ For monthly
data, if we wish to consider “the same month last year,” the notation is
$B^{12}y_{t}$ = $y_{t-12}$.

The backward shift operator is convenient for describing the process of
*differencing*. A first difference can be written as
$$y'_{t} = y_{t} - y_{t-1} = y_t - By_{t} = (1 - B)y_{t}\: .$$ Note that
a first difference is represented by $(1 - B)$. Similarly, if
second-order differences have to be computed, then:
$$y''_{t} = y_{t} - 2y_{t - 1} + y_{t - 2} = (1-2B+B^2)y_t = (1 - B)^{2} y_{t}\: .$$
In general, a $d$th-order difference can be written as
$$(1 - B)^{d} y_{t}.$$

Backshift notation is very useful when combining differences as the
operator can be treated using ordinary algebraic rules. In particular,
terms involving $B$ can be multiplied together.

For example, a seasonal difference followed by a first difference can be
written as

$$\begin{aligned}
(1-B)(1-B^m)y_t &= (1 - B - B^m + B^{m+1})y_t \\
&= y_t-y_{t-1}-y_{t-m}+y_{t-m-1},\end{aligned}$$

the same result we obtained earlier.

##Autoregressive models

In a multiple regression model, we forecast the variable of interest
using a linear combination of predictors. In an autoregression model, we
forecast the variable of interest using a linear combination of *past
values of the variable*. The term *auto*regression indicates that it is
a regression of the variable against itself.

Thus an autoregressive model of order $p$ can be written as
$$y_{t} = c + \phi_{1}y_{t-1} + \phi_{2}y_{t-2} + \dots + \phi_{p}y_{t-p} +
e_{t},$$ where $e_t$ is white noise. This is like a multiple regression
but with *lagged values* of $y_t$ as predictors. We refer to this as an
**AR($p$) model**.

Autoregressive models are remarkably flexible at handling a wide range
of different time series patterns. The two series in Figure \@ref(fig-8-arp)
show series from an AR(1) model and an AR(2) model. Changing the
parameters $\phi_1,\dots,\phi_p$ results in different time series
patterns. The variance of the error term $e_t$ will only change the
scale of the series, not the patterns.

![Two examples of data from autoregressive models with different
parameters. Left: AR(1) with $y_t = 18 -0.8y_{t-1} + e_t$. Right: AR(2)
with $y_t = 8 + 1.3y_{t-1}-0.7y_{t-2}+e_t$. In both cases, $e_t$ is
normally distributed white noise with mean zero and variance one.](arp)

\@ref(fig-8-arp)

For an AR(1) model:

-   when $\phi_1=0$, $y_t$ is

-   when $\phi_1=1$ and $c=0$, $y_t$ is

-   when $\phi_1=1$ and $c\ne0$, $y_t$ is

-   when $\phi_1<0$, $y_t$ tends to

We normally restrict autoregressive models to stationary data, and then
some constraints on the values of the parameters are required.

-   For an AR(1) model:   $-1 < \phi_1 < 1$.

-   For an AR(2) model:   $-1 < \phi_2 < 1$,   $\phi_1+\phi_2 < 1$,  

When $p\ge3$ the restrictions are much more complicated. R takes care of
these restrictions when estimating a model.

#Moving average models

![Two examples of data from moving average models with different
parameters. Left: MA(1) with $y_t = 20 + e_t + 0.8e_{t-1}$. Right: MA(2)
with $y_t = e_t- e_{t-1}+0.8e_{t-2}$. In both cases, $e_t$ is normally
distributed white noise with mean zero and variance one.](maq)

\@ref(fig-8-maq)

Rather than use past values of the forecast variable in a regression, a
moving average model uses past forecast errors in a regression-like
model.
$$y_{t} = c + e_t + \theta_{1}e_{t-1} + \theta_{2}e_{t-2} + \dots + \theta_{q}e_{t-q},$$
where $e_t$ is white noise. We refer to this as an **MA($q$) model**. Of
course, we do not *observe* the values of $e_t$, so it is not really
regression in the usual sense.

Notice that each value of $y_t$ can be thought of as a weighted moving
average of the past few forecast errors. However, moving average
*models* should not be confused with moving average *smoothing* we
discussed in Chapter \@ref(ch6). A moving average model is used for
forecasting future values while moving average smoothing is used for
estimating the trend-cycle of past values.

Figure \@ref(fig-8-maq) shows some data from an MA(1) model and an MA(2)
model. Changing the parameters $\theta_1,\dots,\theta_q$ results in
different time series patterns. As with autoregressive models, the
variance of the error term $e_t$ will only change the scale of the
series, not the patterns.

It is possible to write any stationary AR($p$) model as an MA($\infty$)
model. For example, using repeated substitution, we can demonstrate this
for an AR(1) model:\@ref(ar1mainfty)

$$\begin{aligned}
y_t &= \phi_1y_{t-1} + e_t\\
&= \phi_1(\phi_1y_{t-2} + e_{t-1}) + e_t\\
&= \phi_1^2y_{t-2} + \phi_1 e_{t-1} + e_t\\
&= \phi_1^3y_{t-3} + \phi_1^2e_{t-2} + \phi_1 e_{t-1} + e_t\\
&\text{etc.}\end{aligned}$$

Provided $-1 < \phi_1 < 1$, the value of $\phi_1^k$ will get smaller as
$k$ gets larger. So eventually we obtain
$$y_t = e_t + \phi_1 e_{t-1} + \phi_1^2 e_{t-2} + \phi_1^3 e_{t-3} + \cdots,$$
an MA($\infty$) process.

The reverse result holds if we impose some constraints on the MA
parameters. Then the MA model is called “invertible”. That is, that we
can write any invertible MA($q$) process as an AR($\infty$) process.

Invertible models are not simply to enable us to convert from MA models
to AR models. They also have some mathematical properties that make them
easier to use in practice.

The invertibility constraints are similar to the stationarity
constraints.

-   For an MA(1) model:   $-1<\theta_1<1$.

-   For an MA(2) model:   $-1<\theta_2<1$,   $\theta_2+\theta_1 >-1$,  
    $\theta_1 -\theta_2 < 1$.

More complicated conditions hold for $q\ge3$. Again, R will take care of
these constraints when estimating the models.

##Non-seasonal ARIMA models

If we combine differencing with autoregression and a moving average
model, we obtain a non-seasonal ARIMA model. ARIMA is an acronym for
AutoRegressive Integrated Moving Average model (“integration” in this
context is the reverse of differencing). The full model can be written
as
$$\label{eq-8-arima}
y'_{t} = c + \phi_{1}y'_{t-1} + \cdots + \phi_{p}y'_{t-p} 
 + \theta_{1}e_{t-1} + \cdots + \theta_{q}e_{t-q} + e_{t},
$$

where $y'_{t}$ is the differenced series (it may have been differenced
more than once). The “predictors” on the right hand side include both
lagged values of $y_t$ and lagged errors. We call this an
**ARIMA($p, d, q$) model**, where

  ----------------------------------------------
  $p =$ order of the autoregressive part;
  $d =$ degree of first differencing involved;
  $q =$ order of the moving average part.
  ----------------------------------------------

The same stationarity and invertibility conditions that are used for
autoregressive and moving average models apply to this ARIMA model.

Once we start combining components in this way to form more complicated
models, it is much easier to work with the backshift notation. Then
equation  can be written as $$ =0.1cm

<span>c c c c</span> (1-~1~B - - ~p~ B^p^) & (1-B)^d^ y~t~ &= &c + (1 +
~1~ B + + ~q~ B^q^)e~t~\
 & & &\
 & to 0cm & &\

$$

Selecting appropriate values for $p$, $d$ and $q$ can be difficult. The
`auto.arima()` function in R will do it for you automatically. Later in
this chapter, we will learn how the function works, and some methods for
choosing these values yourself.

Many of the models we have already discussed are special cases of the
ARIMA model as shown in the following table.

--------------------------------------
White noise | ARIMA(0,0,0) 
Random walk | ARIMA(0,1,0) with no constant
Random walk with drift | ARIMA(0,1,0) with a constant
Autoregression | ARIMA($p$,0,0)
Moving average | ARIMA(0,0,$q$)
-----------------------------------------

[US personal consumption]\@ref(sec-8-usconsumption)
Figure \@ref(fig-8-usconsumption) shows quarterly percentage changes in US
consumption expenditure. Although it is quarterly data, there appears to
be no seasonal pattern, so we will fit a non-seasonal ARIMA model.

![Quarterly percentage change in US consumption
expenditure.](usconsumption)

\@ref(fig-8-usconsumption)

The following R code was used to automatically select a model.

\> fit \<- auto.arima(usconsumption[,1], seasonal=FALSE)

ARIMA(0,0,3) with non-zero mean Coefficients: ma1 ma2 ma3 intercept
0.2542 0.2260 0.2695 0.7562 s.e. 0.0767 0.0779 0.0692 0.0844

sigma^2^ estimated as 0.3856: log likelihood=-154.73 AIC=319.46
AICc=319.84 BIC=334.96

This is an ARIMA(0,0,3) or MA(3) model:
$$y_t = 0.756 + e_t + 0.254 e_{t-1} + 0.226 e_{t-2} + 0.269 e_{t-3},$$
where $e_t$ is white noise with standard deviation
$0.62 = \sqrt{0.3856}$. Forecasts from the model are shown in
Figure \@ref(fig-8-usconsumptionf).

![Forecasts of quarterly percentage change in US consumption
expenditure.](usconsumptionf)

\@ref(fig-8-usconsumptionf)

plot(forecast(fit,h=10),include=80)

### Understanding ARIMA models  {-}{#understanding-arima-models .unnumbered}

The `auto.arima()` function is very useful, but anything automated can
be a little dangerous, and it is worth understanding something of the
behaviour of the models even when you rely on an automatic procedure to
choose the model for you.

The constant $c$ has an important effect on the long-term forecasts
obtained from these models.

-   If $c=0$ and $d=0$, the long-term forecasts will go to zero.
-   If $c=0$ and $d=1$, the long-term forecasts will go to a non-zero
    constant.
-   If $c=0$ and $d=2$, the long-term forecasts will follow a straight
    line.
-   If $c\ne0$ and $d=0$, the long-term forecasts will go to the mean of
    the data.
-   If $c\ne0$ and $d=1$, the long-term forecasts will follow a straight
    line.
-   If $c\ne0$ and $d=2$, the long-term forecasts will follow a
    quadratic trend.

The value of $d$ also has an effect on the prediction intervals --- the
higher the value of $d$, the more rapidly the prediction intervals
increase in size. For $d=0$, the long-term forecast standard deviation
will go to the standard deviation of the historical data, so the
prediction intervals will all be essentially the same.

This behaviour is seen in Figure \@ref(fig-8-usconsumptionf) where $d=0$ and
$c\ne 0$. In this figure, the prediction intervals are the same for the
last few forecast horizons, and the point forecasts are equal to the
mean of the data.

The value of $p$ is important if the data show cycles. To obtain cyclic
forecasts, it is necessary to have $p\ge2$ along with some additional
conditions on the parameters. For an AR(2) model, cyclic behaviour
occurs if $\phi_1^2+4\phi_2<0$. In that case, the average period of the
cycles is[^2]
$$\frac{2\pi}{\text{arc cos}(-\phi_1(1-\phi_2)/(4\phi_2))}.$$

### ACF and PACF plots  {-}{#acf-and-pacf-plots .unnumbered}

It is usually not possible to tell, simply from a time plot, what values
of $p$ and $q$ are appropriate for the data. However, it is sometimes
possible to use the ACF plot, and the closely related PACF plot, to
determine appropriate values for $p$ and $q$.

Recall that an ACF plot shows the autocorrelations which measure the
relationship between $y_t$ and $y_{t-k}$ for different values of $k$.
Now if $y_t$ and $y_{t-1}$ are correlated, then $y_{t-1}$ and $y_{t-2}$
must also be correlated. But then $y_t$ and $y_{t-2}$ might be
correlated, simply because they are both connected to $y_{t-1}$, rather
than because of any new information contained in $y_{t-2}$ that could be
used in forecasting $y_t$.

To overcome this problem, we can use **partial autocorrelations**. These
measure the <span>relationship</span> between $y_{t}$ and $y_{t-k}$
after removing the effects of other time lags --- $1, 2, 3, \dots, k - 1$.
So the first partial autocorrelation is identical to the first
autocorrelation, because there is nothing between them to remove. The
partial autocorrelations for lags 2, 3 and greater are calculated as
follows:

$$\begin{aligned}
\alpha_k&= \text{$k$th partial autocorrelation coefficient}\\
&= \text{the estimate of $\phi_k$ in the autoregression model}\\
& \hspace*{0.8cm} y_t = c + \phi_1 y_{t-1} + \phi_2 y_{t-2} + \dots + \phi_k y_{t-k} + e_t.\end{aligned}$$

Varying the number of terms on the right hand side of this
autoregression model gives $\alpha_k$ for different values of $k$. (In
practice, there are more efficient algorithms for computing $\alpha_k$
than fitting all these autoregressions, but they give the same results.)

Figure \@ref(fig-8-usconsumptionacf) shows the ACF and PACF plots for the US
consumption data shown in Figure \@ref(fig-8-usconsumption).

The partial autocorrelations have the same critical values of
$\pm 1.96/\sqrt{T}$ as for ordinary autocorrelations, and these are
typically shown on the plot as in Figure \@ref(fig-8-usconsumptionacf).

![ACF and PACF of quarterly percentage change in US consumption. A
convenient way to produce a time plot, ACF plot and PACF plot in one
command is to use the `tsdisplay` function in R.](usconsumptionacf)

\@ref(fig-8-usconsumptionacf)

par(mfrow=c(1,2)) Acf(usconsumption[,1],main=“”)
Pacf(usconsumption[,1],main=“”)

If the data are from an ARIMA($p$,$d$,0) or ARIMA(0,$d$,$q$) model, then
the ACF and PACF plots can be helpful in determining the value of $p$ or
$q$. If both $p$ and $q$ are positive, then the plots do not help in
finding suitable values of $p$ and $q$.

The data may follow an ARIMA($p$,$d$,0) model if the ACF and PACF plots
of the differenced data show the following patterns:

-   the ACF is exponentially decaying or sinusoidal;

-   there is a significant spike at lag $p$ in PACF, but none beyond lag
    $p$.

The data may follow an ARIMA(0,$d$,$q$) model if the ACF and PACF plots
of the differenced data show the following patterns:

-   the PACF is exponentially decaying or sinusoidal;

-   there is a significant spike at lag $q$ in ACF, but none beyond lag
    $q$.

In Figure \@ref(fig-8-usconsumptionacf), we see that there are three spikes
in the ACF and then no significant spikes thereafter (apart from one
just outside the bounds at lag 14). In the PACF, there are three spikes
decreasing with the lag, and then no significant spikes thereafter
(apart from one just outside the bounds at lag 8). We can ignore one
significant spike in each plot if it is just outside the limits, and not
in the first few lags. After all, the probability of a spike being
significant by chance is about one in twenty, and we are plotting 21
spikes in each plot. The pattern in the first three spikes is what we
would expect from an ARIMA(0,0,3) as the PACF tends to decay
exponentially. So in this case, the ACF and PACF lead us to the same
model as was obtained using the automatic procedure.

## Estimation and order selection

### Maximum likelihood estimation  {-}

Once the model order has been identified (i.e., the values of $p$, $d$
and $q$), we need to estimate the parameters $c$, $\phi_1,\dots,\phi_p$,
$\theta_1,\dots,\theta_q$. When R estimates the ARIMA model, it uses
*maximum likelihood estimation* (MLE). This technique finds the values
of the parameters which maximize the probability of obtaining the data
that we have observed. For ARIMA models, MLE is very similar to the
*least squares* estimates that would be obtained by minimizing
$$\sum_{t=1}^Te_t^2.$$ (For the regression models considered in
Chapters \@ref(ch4) and \@ref(ch5), MLE gives exactly the same parameter estimates
as least squares estimation.) Note that ARIMA models are much more
complicated to estimate than regression models, and different software
will give slightly different answers as they use different methods of
estimation, or different estimation algorithms.

In practice, R will report the value of the *log likelihood* of the
data; that is, the logarithm of the probability of the observed data
coming from the estimated model. For given values of $p$, $d$ and $q$, R
will try to maximize the log likelihood when finding parameter
estimates.

### Information Criteria  {-}{#information-criteria .unnumbered}

Akaike’s Information Criterion (AIC), which was useful in selecting
predictors for regression, is also useful for determining the order of
an ARIMA model. It can be written as
$$\text{AIC} = -2 \log(L) + 2(p+q+k+1),$$ where $L$ is the likelihood of
the data, $k=1$ if $c\ne0$ and $k=0$ if $c=0$. Note that the last term
in parentheses is the number of parameters in the model (including
$\sigma^2$, the variance of the residuals).

For ARIMA models, the corrected AIC can be written as
$$\AICc = \text{AIC} + \frac{2(p+q+k+1)(p+q+k+2)}{T-p-q-k-2}.$$ and the
Bayesian Information Criterion can be written as
$$\text{BIC} = \text{AIC} + [\log(T)-2](p+q+k+1).$$ Good models are
obtained by minimizing either the AIC,  or BIC. Our preference is to use
the .

##ARIMA modelling in R

### How does `auto.arima()` work? {-}

The `auto.arima()` function in R uses a variation of the Hyndman and
Khandakar algorithm[^3] which combines unit root tests, minimization of
the  and MLE to obtain an ARIMA model. The algorithm follows these
steps.

<span>p</span> **Hyndman-Khandakar algorithm**\
**for automatic ARIMA modelling**\

1.  The number of differences $d$ is determined using repeated KPSS
    tests.

2.  The values of $p$ and $q$ are then chosen by minimizing the  after
    differencing the data $d$ times. Rather than considering every
    possible combination of $p$ and $q$, the algorithm uses a stepwise
    search to traverse the model space.

    (a) The best initial model (with smallest ) <span>is selected from
        the following four</span>:\

        ARIMA$(2,d,2)$,\

        ARIMA$(0,d,0)$,\

        ARIMA$(1,d,0)$,\

        ARIMA$(0,d,1)$.\
        If $d=0$ then the constant $c$ is included; if $d\ge1$ then the
        constant $c$ is set to zero. This is called the “current model”.

    (b) Variations on the current model are considered:

        -   vary $p$ and/or $q$ from the current model by
            <span>$\pm1$</span>;

        -   include/exclude $c$ from the current model.

        The best model considered so far (either the current model, or
        one of these variations) becomes the new current model.

    (c) Repeat Step 2(b) until no lower  can be found.

\

The arguments to `auto.arima()` provide for many variations on the
algorithm. What is described here is the default behaviour.

### Choosing your own model  {-}{#choosing-your-own-model .unnumbered}

If you want to choose the model yourself, use the `Arima()` function in
R. For example, to fit the ARIMA(0,0,3) model to the US consumption
data, the following commands can be used.

fit \<- Arima(usconsumption[,1], order=c(0,0,3))

There is another function `arima()` in R which also fits an ARIMA model.
However, it does not allow for the constant $c$ unless $d=0$, and it
does not return everything required for the `forecast()` function.
Finally, it does not allow the estimated model to be applied to new data
(which is useful for checking forecast accuracy). Consequently, it is
recommended that you use `Arima()` instead.

### Modelling procedure  {-}{#modelling-procedure .unnumbered}

When fitting an ARIMA model to a set of time series data, the following
procedure provides a useful general approach.

1.  Plot the data. Identify any unusual observations.

2.  If necessary, transform the data (using a Box-Cox transformation) to
    stabilize the variance.

3.  If the data are non-stationary: take first differences of the data
    until the data are stationary.

4.  Examine the ACF/PACF: Is an AR($p$) or MA($q$) model appropriate?

5.  Try your chosen model(s), and use the  to search for a better model.

6.  Check the residuals from your chosen model by plotting the ACF of
    the residuals, and doing a portmanteau test of the residuals. If
    they do not look like white noise, try a modified model.

7.  Once the residuals look like white noise, calculate forecasts.

The automated algorithm only takes care of steps 3--5. So even if you use
it, you will still need to take care of the other steps yourself.

The process is summarized in Figure \@ref(fig-8-diagram).

= [rectangle, draw, fill=blue!20, text width=3.8cm, text centered,
rounded corners, minimum height=4em]

[!p]

[node distance = 1.4cm, auto,scale=0.9]

[every node/.style=block] (plot) <span> 1. Plot the data. Identify
unusual observations. Understand patterns.</span>; (boxcox) <span> 2. If
necessary, use a Box-Cox transformation to stabilize the
variance.</span>; (expert) <span> Select model order yourself.</span>;
(auto) <span> Use automated algorithm.</span>; (differencing) <span> 3.
If necessary, difference the data until it appears stationary. Use
unit-root tests if you are unsure.</span>; (acf) <span> 4. Plot the
ACF/PACF of the differenced data and try to determine possible candidate
models.</span>; (aic) <span> 5. Try your chosen model(s) and use the  to
search for a better model.</span>; (residuals) <span> 6. Check the
residuals from your chosen model by plotting the ACF of the residuals,
and doing a portmanteau test of the residuals.</span>; (autoarima)
<span> Use `auto.arima()` to find the best ARIMA model for your time
series.</span>; (wn) <span> Do the residuals look like white
noise?</span>; (forecast) <span> 7. Calculate forecasts.</span>;

[every path/.style=line] (plot) -- (boxcox); (boxcox) -- (auto); (boxcox)
-- (expert); (differencing) -- (acf); (acf) -- (aic); (aic) |- (residuals);
(autoarima) |- (residuals); (auto) -- (autoarima); (expert) --
(differencing); (residuals) -- (wn); (wn) -- node
<span>yes</span>(forecast); (wn) -| node [pos=.03] <span>no</span>
(-8.1,-7.556) ; (-8.1,-7.556) -- (acf);

\@ref(fig-8-diagram)

[Seasonally adjusted electrical equipment orders] We will apply this
procedure to the seasonally adjusted electrical equipment orders data
shown in Figure \@ref(fig-8-ee1).

![Seasonally adjusted electrical equipment orders index in the Euro
area.](ee1)

\@ref(fig-8-ee1)

eeadj \<- seasadj(stl(elecequip, s.window=“periodic”)) plot(eeadj)

![Time plot and ACF and PACF plots for differenced seasonally adjusted
electrical equipment data.](ee2)

\@ref(fig-8-ee2)

tsdisplay(diff(eeadj), main=“”)

1.  The time plot shows some sudden changes, particularly the big drop
    in 2008/2009. These changes are due to the global economic
    environment. Otherwise there is nothing unusual about the time plot
    and there appears to be no need to do any data adjustments.

2.  There is no evidence of changing variance, so we will not do a
    Box-Cox transformation.

3.  The data are clearly non-stationary as the series wanders up and
    down for long periods. Consequently, we will take a first difference
    of the data. The differenced data are shown in Figure \@ref(fig-8-ee2).
    These look stationary, and so we will not consider further
    differences.

4.  The PACF shown in Figure \@ref(fig-8-ee2) is suggestive of an AR(3)
    model. So an initial candidate model is an ARIMA(3,1,0). There are
    no other obvious candidate models.

5.  We fit an ARIMA(3,1,0) model along with variations including
    ARIMA(4,1,0), ARIMA(2,1,0), ARIMA(3,1,1), etc. Of these, the
    ARIMA(3,1,1) has a slightly smaller  value.

    \> fit \<- Arima(eeadj, order=c(3,1,1)) \> summary(fit) Series:
    eeadj ARIMA(3,1,1)

    Coefficients: ar1 ar2 ar3 ma1 0.0519 0.1191 0.3730 -0.4542 s.e.
    0.1840 0.0888 0.0679 0.1993

    sigma^2^ estimated as 9.532: log likelihood=-484.08 AIC=978.17
    AICc=978.49 BIC=994.4

6.  The ACF plot of the residuals from the ARIMA(3,1,1) model shows all
    correlations within the threshold limits indicating that the
    residuals are behaving like white noise. A portmanteau test returns
    a large p-value, also suggesting the residuals are white noise.

    Acf(residuals(fit)) Box.test(residuals(fit), lag=24, fitdf=4,
    type=“Ljung”)

7.  Forecasts from the chosen model are shown in Figure \@ref(fig-8-ee4).

![Forecasts for the seasonally adjusted electrical orders index.](ee4)

\@ref(fig-8-ee4)

plot(forecast(fit))

If we had used the automated algorithm instead, we would have obtained
exactly the same model in this example.

### Understanding constants in R {-}

A non-seasonal ARIMA model can be written as

$$\label{eq:c}
(1-\phi_1B - \cdots - \phi_p B^p)(1-B)^d y_t = c + (1 + \theta_1 B + \cdots + \theta_q B^q)e_t$$

or equivalently as

$$\label{eq:mu}
(1-\phi_1B - \cdots - \phi_p B^p)(1-B)^d (y_t - \mu t^d/d!) = (1 + \theta_1 B + \cdots + \theta_q B^q)e_t,$$

where $c = \mu(1-\phi_1 - \cdots - \phi_p )$ and $\mu$ is the mean of
$(1-B)^d y_t$. R uses the parametrization of equation .

Thus, the inclusion of a constant in a non-stationary ARIMA model is
equivalent to inducing a polynomial trend of order $d$ in the forecast
function. (If the constant is omitted, the forecast function includes a
polynomial trend of order $d-1$.) When $d=0$, we have the special case
that $\mu$ is the mean of $y_t$.

arima()
:   By default, the `arima()` command in R sets $c=\mu=0$ when $d>0$ and
    provides an estimate of $\mu$ when $d=0$. The parameter $\mu$ is
    called the “intercept” in the R output. It will be close to the
    sample mean of the time series, but usually not identical to it as
    the sample mean is not the maximum likelihood estimate when $p+q>0$.

    The `arima()` command has an argument `include.mean` which only has
    an effect when $d=0$ and is `TRUE` by default. Setting
    `include.mean=FALSE` will force $\mu=0$.

Arima()
:   The `Arima()` command from the forecast package provides more
    flexibility on the inclusion of a constant. It has an argument
    `include.mean` which has identical functionality to the
    corresponding argument for `arima()`. It also has an argument
    `include.drift` which allows $\mu\ne0$ when $d=1$. For $d>1$, no
    constant is allowed as a quadratic or higher order trend is
    particularly dangerous when forecasting. The parameter $\mu$ is
    called the “drift” in the R output when $d=1$.

    There is also an argument `include.constant` which, if `TRUE`, will
    set `include.mean=TRUE` if $d=0$ and `include.drift=TRUE` when
    $d=1$. If `include.constant=FALSE`, both `include.mean` and
    `include.drift` will be set to `FALSE`. If `include.constant` is
    used, the values of `include.mean=TRUE` and `include.drift=TRUE` are
    ignored.

auto.arima()
:   The `auto.arima()` function automates the inclusion of a constant.
    By default, for $d=0$ or $d=1$, a constant will be included if it
    improves the  value; for $d>1$ the constant is always omitted. If
    `allowdrift=FALSE` is specified, then the constant is only allowed
    when $d=0$.

##Forecasting

### Point forecasts  {-}}

Although we have calculated forecasts from the ARIMA models in our
examples, we have not yet explained how they are obtained. Point
forecasts can be calculated using the following three steps.

1.  Expand the ARIMA equation so that $y_t$ is on the left hand side and
    all other terms are on the right.

2.  Rewrite the equation by replacing $t$ by $T+h$.

3.  On the right hand side of the equation, replace future observations
    by their forecasts, future errors by zero, and past errors by the
    corresponding residuals.

Beginning with $h=1$, these steps are then repeated for $h=2,3,\dots$
until all forecasts have been calculated.

The procedure is most easily understood via an example. We will
illustrate it using the ARIMA(3,1,1) model fitted in the previous
section. The model can be written as follows
$$(1-\hat{\phi}_1B -\hat{\phi}_2B^2-\hat{\phi}_3B^3)(1-B) y_t = (1+\hat{\theta}_1B)e_{t},$$
where $\hat{\phi}_1=0.0519$, $\hat{\phi}_2=0.1191$,
$\hat{\phi}_3=0.3730 $ and $\hat{\theta}_1=-0.4542$. Then we expand the
left hand side to obtain
$$\left[1-(1+\hat{\phi}_1)B +(\hat{\phi}_1-\hat{\phi}_2)B^2 + (\hat{\phi}_2-\hat{\phi}_3)B^3 +\hat{\phi}_3B^4\right] y_t = (1+\hat{\theta}_1B)e_{t},$$
and applying the backshift operator gives
$$y_t - (1+\hat{\phi}_1)y_{t-1} +(\hat{\phi}_1-\hat{\phi}_2)y_{t-2} + (\hat{\phi}_2-\hat{\phi}_3)y_{t-3} +\hat{\phi}_3y_{t-4} = e_t+\hat{\theta}_1e_{t-1}.$$
Finally, we move all terms other than $y_t$ to the right hand side:

$$\label{arima301f}
y_t = (1+\hat{\phi}_1)y_{t-1} -(\hat{\phi}_1-\hat{\phi}_2)y_{t-2} - (\hat{\phi}_2-\hat{\phi}_3)y_{t-3} -\hat{\phi}_3y_{t-4} + e_t+\hat{\theta}_1e_{t-1}.$$

This completes the first step. While the equation now looks like an
ARIMA(4,0,1), it is still the same ARIMA(3,1,1) model we started with.
It cannot be considered an ARIMA(4,0,1) because the coefficients do not
satisfy the stationarity conditions.

For the second step, we replace $t$ by $T+1$ in :
$$y_{T+1} = (1+\hat{\phi}_1)y_{T} -(\hat{\phi}_1-\hat{\phi}_2)y_{T-1} - (\hat{\phi}_2-\hat{\phi}_3)y_{T-2} -\hat{\phi}_3y_{T-3} + e_{T+1}+\hat{\theta}_1e_{T}.$$
Assuming we have observations up to time $T$, all values on the right
hand side are known except for $e_{T+1}$ which we replace by zero and
$e_T$ which we replace by the last observed residual $\hat{e}_T$:
$$\hat{y}_{T+1|T} = (1+\hat{\phi}_1)y_{T} -(\hat{\phi}_1-\hat{\phi}_2)y_{T-1} - (\hat{\phi}_2-\hat{\phi}_3)y_{T-2} -\hat{\phi}_3y_{T-3} + \hat{\theta}_1\hat{e}_{T}.$$

A forecast of $y_{T+2}$ is obtained by replacing $t$ by $T+2$ in . All
values on the right hand side will be known at time $T$ except $y_{T+1}$
which we replace by $\hat{y}_{T+1|T}$, and $e_{T+2}$ and $e_{T+1}$, both
of which we replace by zero:
$$\hat{y}_{T+2|T} = (1+\hat{\phi}_1)\hat{y}_{T+1|T} -(\hat{\phi}_1-\hat{\phi}_2)y_{T} - (\hat{\phi}_2-\hat{\phi}_3)y_{T-1} -\hat{\phi}_3y_{T-2}.$$

The process continues in this manner for all future time periods. In
this way, any number of point forecasts can be obtained.

### Forecast intervals  {-}

The calculation of ARIMA forecast intervals is much more difficult, and
the details are largely beyond the scope of this book. We will just give
some simple examples.

The first forecast interval is easily calculated. If $\hat{\sigma}$ is
the standard deviation of the residuals, then a 95% forecast interval is
given by $\hat{y}_{T+1|T} \pm 1.96\hat{\sigma}$. This result is true for
all ARIMA models regardless of their parameters and orders.

Multi-step forecast intervals for ARIMA(0,0,$q$) models are relatively
easy to calculate. We can write the model as
$$y_t = e_t + \sum_{i=1}^q \theta_i e_{t-i}.$$ Then the estimated
forecast variance can be written as
$$v_{T+h|T} = \hat{\sigma}^2 \left[ 1 + \sum_{i=1}^{h-1} \hat{\theta}_i^2\right], \qquad\text{for $h=2,3,\dots$,}$$
and a 95% forecast interval is given by
$\hat{y}_{T+h|T} \pm 1.96\sqrt{v_{T+h|T}}$.

On page , we showed that an AR(1) model can be written as an
MA($\infty$) model. Using this equivalence, the above result for MA($q$)
models can also be used to obtain forecast intervals for AR(1) models.

More general results, and other special cases of multi-step forecast
intervals for an ARIMA($p$,$d$,$q$) model, are given in more advanced
textbooks such as Brockwell and Davis (1991, Section 9.5).

The forecast intervals for ARIMA models are based on assumptions that
the residuals are uncorrelated and normally distributed. If either of
these are assumptions do not hold, then the forecast intervals may be
incorrect. For this reason, always plot the ACF and histogram of the
residuals to check the assumptions before producing forecast intervals.

In general, forecast intervals from ARIMA models will increase as the
forecast horizon increases. For stationary models (i.e., with $d=0$),
they will converge so forecast intervals for long horizons are all
essentially the same. For $d>1$, the forecast intervals will continue to
grow into the future.

As with most forecast interval calculations, ARIMA-based intervals tend
to be too narrow. This occurs because only the variation in the errors
has been accounted for. There is also variation in the parameter
estimates, and in the model order, that has not been included in the
calculation. In addition, the calculation assumes that the historical
patterns that have been modelled will continue into the forecast period.

##Seasonal ARIMA models

So far, we have restricted our attention to non-seasonal data and
non-seasonal ARIMA models. However, ARIMA models are also capable of
modelling a wide range of seasonal data.

A seasonal ARIMA model is formed by including additional seasonal terms
in the ARIMA models we have seen so far. It is written as follows:

<span>cl</span> ARIMA $~\underbrace{(p, d, q)}$ 
$\underbrace{(P, D, Q)_{m}}$ &\

${\uparrow}$

${\uparrow}$\

where $m =$ number of observations per year. We use uppercase notation
for the seasonal parts of the model, and lowercase notation for the
non-seasonal parts of the model.

The seasonal part of the model consists of terms that are very similar
to the non-seasonal components of the model, but they involve backshifts
of the seasonal period. For example, an ARIMA$(1, 1, 1)(1, 1, 1)_{4}$
model (without a constant) is for quarterly data ($m=4$) and can be
written as
$$(1 - \phi_{1}B)~(1 - \Phi_{1}B^{4}) ~(1 - B)~ (1 - B^{4})y_{t} ~= ~
(1 + \theta_{1}B)~ (1 + \Theta_{1}B^{4})e_{t}.$$

(100,25)(-10,0) <span>(21,10)<span>(0,1)<span>18</span></span></span>
<span>(38,22)<span>(0,1)<span>6</span></span></span>
<span>(54,10)<span>(0,1)<span>18</span></span></span>
<span>(83,22)<span>(0,1)<span>6</span></span></span>
<span>(100,10)<span>(0,1)<span>18</span></span></span>
<span>(-10,17)<span>$\left(\begin{array}{@{}c@{}} \text{Non-seasonal} \\ \text{AR(1)}
 \end{array}\right)$</span></span>
<span>(12,5)<span>$\left(\begin{array}{@{}c@{}} \text{Seasonal} \\ \text{AR(1)}
 \end{array}\right)$</span></span>
<span>(25,17)<span>$\left(\begin{array}{@{}c@{}} \text{Non-seasonal} \\ \text{difference}
 \end{array}\right)$</span></span>
<span>(45,5)<span>$\left(\begin{array}{@{}c@{}} \text{Seasonal} \\ \text{difference}
 \end{array}\right)$</span></span>
<span>(70,17)<span>$\left(\begin{array}{@{}c@{}} \text{Non-seasonal} \\ \text{MA(1)}
 \end{array}\right)$</span></span>
<span>(95,5)<span>$\left(\begin{array}{@{}c@{}} \text{Seasonal} \\ \text{MA(1)}
 \end{array}\right)$</span></span>

The additional seasonal terms are simply multiplied with the
non-seasonal terms.

### ACF {-}/PACF {#acfpacf .unnumbered}

The seasonal part of an AR or MA model will be seen in the seasonal lags
of the PACF and ACF. For example, an ARIMA(0,0,0)(0,0,1)$_{12}$ model
will show:

-   a spike at lag 12 in the ACF but no other significant spikes.

-   The PACF will show exponential decay in the seasonal lags; that is,
    at lags 12, 24, <span>36, ….</span>

Similarly, an ARIMA(0,0,0)(1,0,0)$_{12}$ model will show:

-   exponential decay in the seasonal lags of the ACF

-   a single significant spike at lag 12 in the PACF.

In considering the appropriate seasonal orders for an ARIMA model,
restrict attention to the seasonal lags.

The modelling procedure is almost the same as for non-seasonal data,
except that we need to select seasonal AR and MA terms as well as the
non-seasonal components of the model. The process is best illustrated
via examples.

[European quarterly retail trade]

We will describe the seasonal ARIMA modelling procedure using quarterly
European retail trade data from 1996 to 2011. The data are plotted in
Figure \@ref(fig-8-euretail1).

![Quarterly retail trade index in the Euro area (17 countries),
1996--2011, covering wholesale and retail trade, and repair of motor
vehicles and motorcycles. (Index: 2005 = 100).](euretail1)

\@ref(fig-8-euretail1)

plot(euretail, ylab=“Retail index”, xlab=“Year”)

The data are clearly non-stationary, with some seasonality, so we will
first take a seasonal difference. The seasonally differenced data are
shown in Figure \@ref(fig-8-euretail2). These also appear to be
non-stationary, and so we take an additional first difference, shown in
Figure \@ref(fig-8-euretail3).

![Seasonally differenced European retail trade index.](euretail2)

\@ref(fig-8-euretail2)

tsdisplay(diff(euretail,4))

![Double differenced European retail trade index.](euretail3)

\@ref(fig-8-euretail3)

tsdisplay(diff(diff(euretail,4)))

Our aim now is to find an appropriate ARIMA model based on the ACF and
PACF shown in Figure \@ref(fig-8-euretail3). The significant spike at lag 1
in the ACF suggests a non-seasonal MA(1) component, and the significant
spike at lag 4 in the ACF suggests a seasonal MA(1) component.
Consequently, we begin with an ARIMA(0,1,1)(0,1,1)$_4$ model, indicating
a first and seasonal difference, and non-seasonal and seasonal MA(1)
components. The residuals for the fitted model are shown in
Figure \@ref(fig-8-euretail4). (By analogous logic applied to the PACF, we
could also have started with an ARIMA(1,1,0)(1,1,0)$_4$ model.)

![Residuals from the fitted ARIMA(0,1,3)(0,1,1)$_4$ model for the
European retail trade index data.](euretail4)

\@ref(fig-8-euretail4)

fit \<- Arima(euretail, order=c(0,1,1), seasonal=c(0,1,1))
tsdisplay(residuals(fit))

Both the ACF and PACF show significant spikes at lag 2, and almost
significant spikes at lag 3, indicating some additional non-seasonal
terms need to be included in the model. The  of the
ARIMA(0,1,2)(0,1,1)$_4$ model is 74.36, while that for the
ARIMA(0,1,3)(0,1,1)$_4$ model is 68.53. We tried other models with AR
terms as well, but none that gave a smaller  value. Consequently, we
choose the ARIMA(0,1,3)(0,1,1)$_4$ model. Its residuals are plotted in
Figure \@ref(fig-8-euretail5). All the spikes are now within the significance
limits, and so the residuals appear to be white noise. A Ljung-Box test
also shows that the residuals have no remaining autocorrelations.

![Residuals from the fitted ARIMA(0,1,3)(0,1,1)$_4$ model for the
European retail trade index data.](euretail5)

\@ref(fig-8-euretail5)

fit3 \<- Arima(euretail, order=c(0,1,3), seasonal=c(0,1,1)) res \<-
residuals(fit3) tsdisplay(res) Box.test(res, lag=16, fitdf=4,
type=“Ljung”)

So we now have a seasonal ARIMA model that passes the required checks
and is ready for forecasting. Forecasts from the model for the next
three years are shown in Figure \@ref(fig-8-euretail6). Notice how the
forecasts follow the recent trend in the data (this occurs because of
the double differencing). The large and rapidly increasing prediction
intervals show that the retail trade index could start increasing or
decreasing at any time --- while the point forecasts trend downwards, the
prediction intervals allow for the data to trend upwards during the
forecast period.

![Forecasts of the European retail trade index data using the
ARIMA(0,1,3)(0,1,1)$_4$ model. 80% and 95% prediction intervals are
shown.](euretail6)

\@ref(fig-8-euretail6)

plot(forecast(fit3, h=12))

We could have used `auto.arima()` to do most of this work for us. It
would have given the following result.

\> auto.arima(euretail) ARIMA(1,1,1)(0,1,1)\@ref(4)

Coefficients: ar1 ma1 sma1 0.8828 -0.5208 -0.9704 s.e. 0.1424 0.1755
0.6792

sigma^2^ estimated as 0.1411: log likelihood=-30.19 AIC=68.37 AICc=69.11
BIC=76.68

Notice that it has selected a different model (with a larger  value).
`auto.arima()` takes some short-cuts in order to speed up the
computation and will not always give the best model. You can turn the
short-cuts off and then it will sometimes return a different model.

\> auto.arima(euretail, stepwise=FALSE, approximation=FALSE)
ARIMA(0,1,3)(0,1,1)\@ref(4)

Coefficients: ma1 ma2 ma3 sma1 0.2625 0.3697 0.4194 -0.6615 s.e. 0.1239
0.1260 0.1296 0.1555

sigma^2^ estimated as 0.1451: log likelihood=-28.7 AIC=67.4 AICc=68.53
BIC=77.78

This time it returned the same model we had identified.

[Cortecosteroid drug sales in Australia]

Our second example is more difficult. We will try to forecast monthly
cortecosteroid drug sales in Australia. These are known as H02 drugs
under the Anatomical Therapeutical Chemical classification scheme.

![Cortecosteroid drug sales in Australia (in millions of scripts per
month). Logged data shown in bottom panel.](h02)

\@ref(fig-8-h02)

lh02 \<- log(h02) par(mfrow=c(2,1)) plot(h02, ylab=“H02 sales (million
scripts)”, xlab=“Year”) plot(lh02, ylab=“Log H02 sales”, xlab=“Year”)

Data from July 1991 to June 2008 are plotted in Figure \@ref(fig-8-h02).
There is a small increase in the variance with the level, and so we take
logarithms to stabilize the variance.

The data are strongly seasonal and obviously non-stationary, and so
seasonal differencing will be used. The seasonally differenced data are
shown in Figure \@ref(fig-8-h02b). It is not clear at this point whether we
should do another difference or not. We decide not to, but the choice is
not obvious.

The last few observations appear to be different (more variable) from
the earlier data. This may be due to the fact that data are sometimes
revised as earlier sales are reported late.

![Seasonally differenced cortecosteroid drug sales in Australia (in
millions of scripts per month).](h02b)

\@ref(fig-8-h02b)

tsdisplay(diff(lh02,12), main=“Seasonally differenced H02 scripts”,
xlab=“Year”)

In the plots of the seasonally differenced data, there are spikes in the
PACF at lags 12 and 24, but nothing at seasonal lags in the ACF. This
may be suggestive of a seasonal AR(2) term. In the non-seasonal lags,
there are three significant spikes in the PACF suggesting a possible
AR(3) term. The pattern in the ACF is not indicative of any simple
model.

Consequently, this initial analysis suggests that a possible model for
these data is an ARIMA(3,0,0)(2,1,0)$_{12}$. We fit this model, along
with some variations on it, and compute their  values which are shown in
the following table.

<span>lc</span> Model &\
ARIMA(3,0,0)(2,1,0)$_{12}$ & $-475.12$\
ARIMA(3,0,1)(2,1,0)$_{12}$ & $-476.31$\
ARIMA(3,0,2)(2,1,0)$_{12}$ & $-474.88$\
ARIMA(3,0,1)(1,1,0)$_{12}$ & $-463.40$\
ARIMA(3,0,1)(0,1,1)$_{12}$ & $-483.67$\
ARIMA(3,0,1)(0,1,2)$_{12}$ & $-485.48$\
ARIMA(3,0,1)(1,1,1)$_{12}$ & $-484.25$\

Of these models, the best is the ARIMA(3,0,1)(0,1,2)$_{12}$ model (i.e.,
it has the smallest  value).

\> fit \<- Arima(h02, order=c(3,0,1), seasonal=c(0,1,2), lambda=0)

ARIMA(3,0,1)(0,1,2)\@ref(12) Box Cox transformation: lambda= 0

Coefficients: ar1 ar2 ar3 ma1 sma1 sma2 -0.1603 0.5481 0.5678 0.3827
-0.5222 -0.1768 s.e. 0.1636 0.0878 0.0942 0.1895 0.0861 0.0872

sigma^2^ estimated as 0.004145: log likelihood=250.04 AIC=-486.08
AICc=-485.48 BIC=-463.28

The residuals from this model are shown in Figure \@ref(fig-8-h02res). There
are significant spikes in both the ACF and PACF, and the model fails a
Ljung-Box test. The model can still be used for forecasting, but the
prediction intervals may not be accurate due to the correlated
residuals.

![Residuals from the ARIMA(3,0,1)(0,1,2)$_{12}$ model applied to the H02
monthly script sales data.](h02res)

\@ref(fig-8-h02res)

tsdisplay(residuals(fit)) Box.test(residuals(fit), lag=36, fitdf=6,
type=“Ljung”)

Next we will try using the automatic ARIMA algorithm. Running
`auto.arima()` with arguments left at their default values led to an
ARIMA(2,1,3)(0,1,1)$_{12}$ model. However, the model still fails a
Ljung-Box test. Sometimes it is just not possible to find a model that
passes all the tests.

Finally, we tried running `auto.arima()` with differencing specified to
be $d=0$ and $D=1$, and allowing larger models than usual. This led to
an ARIMA(4,0,3)(0,1,1)$_{12}$ model, which did pass all the tests.

fit \<- auto.arima(h02, lambda=0, d=0,D=1,max.order=9, stepwise=FALSE,
approximation=FALSE) tsdisplay(residuals(fit)) Box.test(residuals(fit),
lag=36, fitdf=8, type=“Ljung”)

#### Test set evaluation: {#test-set-evaluation .unnumbered}

We will compare some of the models fitted so far using a test set
consisting of the last two years of data. Thus, we fit the models using
data from July 1991 to June 2006, and forecast the script sales for July
2006 -- June 2008. The results are summarised in the following table.

<span>2</span>

<span>@lc@</span> **Model & **RMSE\
ARIMA(3,0,0)(2,1,0)$_{12}$ & 0.0661\
ARIMA(3,0,1)(2,1,0)$_{12}$ & 0.0646\
ARIMA(3,0,2)(2,1,0)$_{12}$ & 0.0645\
ARIMA(3,0,1)(1,1,0)$_{12}$ & 0.0679\
ARIMA(3,0,1)(0,1,1)$_{12}$ & 0.0644\
ARIMA(3,0,1)(0,1,2)$_{12}$ & 0.0622\
ARIMA(3,0,1)(1,1,1)$_{12}$ & 0.0630\
ARIMA(4,0,3)(0,1,1)$_{12}$ & 0.0648\
ARIMA(3,0,3)(0,1,1)$_{12}$ & 0.0640\
ARIMA(4,0,2)(0,1,1)$_{12}$ & 0.0648\
ARIMA(3,0,2)(0,1,1)$_{12}$ & 0.0644\
ARIMA(2,1,3)(0,1,1)$_{12}$ & 0.0634\
ARIMA(2,1,4)(0,1,1)$_{12}$ & 0.0632\
ARIMA(2,1,5)(0,1,1)$_{12}$ & 0.0640\
****

<span>8.8cm</span>

    getrmse <- function(x,h,...)
    {
      train.end <- time(x)[length(x)-h]
      test.start <- time(x)[length(x)-h+1]
      train <- window(x,end=train.end)
      test <- window(x,start=test.start)
      fit <- Arima(train,...)
      fc <- forecast(fit,h=h)
      return(accuracy(fc,test)[2,"RMSE"])
    }

    getrmse(h02,h=24,order=c(3,0,0),seasonal=c(2,1,0),lambda=0)
    getrmse(h02,h=24,order=c(3,0,1),seasonal=c(2,1,0),lambda=0)
    getrmse(h02,h=24,order=c(3,0,2),seasonal=c(2,1,0),lambda=0)
    getrmse(h02,h=24,order=c(3,0,1),seasonal=c(1,1,0),lambda=0)
    getrmse(h02,h=24,order=c(3,0,1),seasonal=c(0,1,1),lambda=0)
    getrmse(h02,h=24,order=c(3,0,1),seasonal=c(0,1,2),lambda=0)
    getrmse(h02,h=24,order=c(3,0,1),seasonal=c(1,1,1),lambda=0)
    getrmse(h02,h=24,order=c(4,0,3),seasonal=c(0,1,1),lambda=0)
    getrmse(h02,h=24,order=c(3,0,3),seasonal=c(0,1,1),lambda=0)
    getrmse(h02,h=24,order=c(4,0,2),seasonal=c(0,1,1),lambda=0)
    getrmse(h02,h=24,order=c(3,0,2),seasonal=c(0,1,1),lambda=0)
    getrmse(h02,h=24,order=c(2,1,3),seasonal=c(0,1,1),lambda=0)
    getrmse(h02,h=24,order=c(2,1,4),seasonal=c(0,1,1),lambda=0)
    getrmse(h02,h=24,order=c(2,1,5),seasonal=c(0,1,1),lambda=0)

The models that have the lowest  values tend to give slightly better
results than the other models, but there is not a large difference.
Also, the only model that passed the residual tests did not give the
best test set RMSE values.

When models are compared using  values, it is important that all models
have the same orders of differencing. However, when comparing models
using a test set, it does not matter how the forecasts were produced ---
the comparisons are always valid. Consequently, in the table above, we
can include some models with only seasonal differencing and some models
with both first and seasonal differencing. But in the earlier table
containing  values, we compared models with only seasonal differencing.

None of the models considered here pass all the residual tests. In
practice, we would normally use the best model we could find, even if it
did not pass all tests.

Forecasts from the ARIMA(3,0,1)(0,1,2)$_{12}$ model (which has the
lowest RMSE value on the test set, and the best AICc value amongst
models with only seasonal differencing and fewer than six parameters)
are shown in the figure below.

![Forecasts from the ARIMA(3,0,1)(0,1,2)$_{12}$ model applied to the H02
monthly script sales data.](h02f)

fit \<- Arima(h02, order=c(3,0,1), seasonal=c(0,1,2), lambda=0)
plot(forecast(fit), ylab=“H02 sales (million scripts)”, xlab=“Year”)

##ARIMA vs ETS

It is a commonly held myth that ARIMA models are more general than
exponential smoothing. While linear exponential smoothing models are all
special cases of ARIMA models, the non-linear exponential smoothing
models have no equivalent ARIMA counterparts. There are also many ARIMA
models that have no exponential smoothing counterparts. In particular,
every ETS model is non-stationary, while ARIMA models can be stationary.

The ETS models with seasonality or non-damped trend or both have two
unit roots (i.e., they need two levels of differencing to make them
stationary). All other ETS models have one unit root (they need one
level of differencing to make them stationary).

The following table gives the equivalence relationships for the two
classes of models.

<span>lll</span> **ETS model** & **ARIMA model** & **Parameters**\
ETS(A,N,N) & ARIMA(0,1,1) & $\theta_1 = \alpha-1$\
ETS(A,A,N) & ARIMA(0,2,2) & $\theta_1 = \alpha+\beta-2$\
&& $\theta_2 = 1-\alpha$\
ETS(A,A,N) & ARIMA(1,1,2) & $\phi_1=\phi$\
&& $\theta_1 = \alpha+\phi\beta-1-\phi$\
&& $\theta_2 = (1-\alpha)\phi$\
ETS(A,N,A) & ARIMA(0,0,$m$)(0,1,0)$_m$\
ETS(A,A,A) & ARIMA(0,1,$m+1$)(0,1,0)$_m$\
ETS(A,A,A) & ARIMA(1,0,$m+1$)(0,1,0)$_m$\

For the seasonal models, there are a large number of restrictions on the
ARIMA parameters

Figure \@ref(wnacf) shows the ACFs for 36 random numbers, 360 random numbers
and for 1,000 random numbers.

(a) Explain the differences among these figures. Do they all indicate
    the data are white noise?

    [!h] !\@ref(image)(wnacfplus)

    \@ref(wnacf)

(b) Why are the critical values at different distances from the mean of
    zero? Why are the autocorrelations different in each figure when
    they each refer to white noise?

A classic example of a non-stationary series is the daily closing IBM
stock prices (data set `ibmclose`). Use R to plot the daily closing
prices for IBM stock and the ACF and PACF. Explain how each plot shows
the series is non-stationary and should be differenced.

For the following series, find an appropriate Box-Cox transformation and
order of differencing in order to obtain stationary data.

(a) `usnetelec`

(b) `usgdp`

(c) `mcopper`

(d) `enplanements`

(e) `visitors`

For the `enplanements` data, write down the differences you chose above
using backshift operator notation.

Use R to simulate and plot some data from simple ARIMA models.

(a) Use the following R code to generate data from an AR(1) model with
    $\phi_{1} = 0.6$ and $\sigma^2=1$. The process starts with $y_1=0$.

    y \<- ts(numeric(100)) e \<- rnorm(100) for(i in 2:100) y\@ref(i) \<-
    0.6\*y\@ref(i-1) + e\@ref(i)

(b) Produce a time plot for the series. How does the plot change as you
    change $\phi_1$?

(c) Write your own code to generate data from an MA(1) model with
    $\theta_{1}  =  0.6$ and $\sigma^2=1$.

(d) Produce a time plot for the series. How does the plot change as you
    change $\theta_1$?

(e) Generate data from an ARMA(1,1) model with $\phi_{1} = 0.6$ and
    $\theta_{1}  = 0.6$ and $\sigma^2=1$.

(f) Generate data from an AR(2) model with $\phi_{1} =-0.8$ and
    $\phi_{2} = 0.3$ and $\sigma^2=1$. (Note that these parameters will
    give a non-stationary series.)

(g) Graph the latter two series and compare them.

Consider the number of women murdered each year (per 100,000 standard
population) in the United States (data set `wmurders`).

(a) By studying appropriate graphs of the series in R, find an
    appropriate ARIMA($p,d,q$) model for these data.

(b) Should you include a constant in the model? Explain.

(c) Write this model in terms of the backshift operator.

(d) Fit the model using R and examine the residuals. Is the model
    satisfactory?

(e) Forecast three times ahead. Check your forecasts by hand to make
    sure you know how they have been calculated.

(f) Create a plot of the series with forecasts and prediction intervals
    for the next three periods shown.

(g) Does `auto.arima` give the same model you have chosen? If not, which
    model do you think is better?

Consider the quarterly number of international tourists to Australia for
the period 1999--2010. (Data set `austourists`.)

(a) Describe the time plot.

(b) What can you learn from the ACF graph?

(c) What can you learn from the PACF graph?

(d) Produce plots of the seasonally differenced data $(1 - B^{4})Y_{t}$.
    What model do these graphs suggest?

(e) Does `auto.arima` give the same model that you chose? If not, which
    model do you think is better?

(f) Write the model in terms of the backshift operator, and then without
    using the backshift operator.

Consider the total net generation of electricity (in billion kilowatt
hours) by the U.S. electric industry (monthly for the period 1985--1996).
(Data set `usmelec`.) In general there are two peaks per year: in
mid-summer and mid-winter.

(a) Examine the 12-month moving average of this series to see what kind
    of trend is involved.

(b) Do the data need transforming? If so, find a suitable
    transformation.

(c) Are the data stationary? If not, find an appropriate differencing
    which yields stationary data.

(d) Identify a couple of ARIMA models that might be useful in describing
    the time series. Which of your models is the best according to their
    AIC values?

(e) Estimate the parameters of your best model and do diagnostic testing
    on the residuals. Do the residuals resemble white noise? If not, try
    to find another ARIMA model which fits better.

(f) Forecast the next 15 years of generation of electricity by the U.S.
    electric industry. Get the latest figures from
    <http://data.is/zgRWCO> to check on the accuracy of your forecasts.

(g) How many years of forecasts do you think are sufficiently accurate
    to be usable?

For the `mcopper` data:

(a) if necessary, find a suitable Box-Cox transformation for the data;

(b) fit a suitable ARIMA model to the transformed data using
    `auto.arima()`;

(c) try some other plausible models by experimenting with the orders
    chosen;

(d) choose what you think is the best model and check the residual
    diagnostics;

(e) produce forecasts of your fitted model. Do the forecasts look
    reasonable?

(f) compare the results with what you would obtain using `ets()` (with
    no transformation).

\@ref(exercise-Q1) Choose one of the following seasonal time series:
`condmilk`, `hsales`, `uselec`

(a) Do the data need transforming? If so, find a suitable
    transformation.

(b) Are the data stationary? If not, find an appropriate differencing
    which yields stationary data.

(c) Identify a couple of ARIMA models that might be useful in describing
    the time series. Which of your models is the best according to their
    AIC values?

(d) Estimate the parameters of your best model and do diagnostic testing
    on the residuals. Do the residuals resemble white noise? If not, try
    to find another ARIMA model which fits better.

(e) Forecast the next 24 months of data using your preferred model.

(f) Compare the forecasts obtained using `ets()`.

For the same time series you used in \@ref(exercise-Q1), try using a
non-seasonal model applied to the seasonally adjusted data obtained from
STL. The `stlf()` function will make the calculations easy (with
`method="arima"`). Compare the forecasts with those obtained in
\@ref(exercise-Q1). Which do you think is the best approach?

[^1]: More precisely, if $\{y_t\}$ is a *stationary* time series, then
    for all $s$, the distribution of $(y_t,\dots,y_{t+s})$ does not
    depend on $t$.

[^2]: arc cos is the inverse cosine function. You should be able to find
    it on your calculator. It may be labelled acos or cos$^{-1}$.

[^3]: `robjhyndman.com/ papers/automatic`

